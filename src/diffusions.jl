module DiffusionAlgorithms
using SparseArrays
using ProgressMeter
using Printf
using Statistics
using Random
using DataFrames
using DataStructures
using SharedArrays
using Distributed
using CSV

import LinearAlgebra.checksquare


function normout!(A::SparseMatrixCSC{Float64,Int64})
    d = sum(A,dims=2) # sum over rows
    # use some internal julia magic
    for i=1:length(A.nzval)
        A.nzval[i] = A.nzval[i] / d[A.rowval[i]]
    end
    return A
end


"""
- `maxresidvol::Int` - the maximum residual volume considered, if this is negative,
then we treat it as infinite.

Returns
-------
(x::Dict{Int,Float64},r::Dict{Int,Float64},flag::Int)
"""
function weighted_ppr_push(A::SparseMatrixCSC{T,Int}, seed::Int,
    alpha::Float64, eps::Float64, maxpush::Int, dvec::Vector{Int}, maxresidvol::Int) where T

    colptr = A.colptr
    rowval = A.rowval
    nzval = A.nzval

    n = size(A,1)

    x = Dict{Int,Float64}()     # Store x, r as dictionaries
    r = Dict{Int,Float64}()     # initialize residual
    Q = Queue{Int}()            # initialize queue
    npush = 0.

    if maxresidvol <= 0
        maxresidvol = typemax(Int)
    end

    rvol = 0

    # TODO handle a generic seed
    r[seed] = 1.
    enqueue!(Q,seed)

    pushcount = 0
    pushvol = 0

    @inbounds while length(Q) > 0 && pushcount <= maxpush
        pushcount += 1
        u = dequeue!(Q)

        du = dvec[u] # get the degree

        pushval = r[u] - 0.5*eps*du
        x[u] = get(x,u,0.0) + (1-alpha)*pushval
        r[u] = 0.5*eps*du

        pushval = pushval*alpha

        for nzi in colptr[u]:(colptr[u+1] - 1)
            pushvol += 1
            v = rowval[nzi]
            dv = dvec[v] # degree of v

            rvold = get(r,v,0.)
            if rvold == 0.
                rvol += dv
            end
            rvnew = rvold + pushval*nzval[nzi]/du

            r[v] = rvnew
            if rvnew > eps*dv && rvold <= eps*dv
                #push!(Q,v)
                enqueue!(Q,v)
            end
        end

        if rvol >= maxresidvol
            return x, r, -2
        end
    end

    if pushcount > maxpush
        return x, r, -1, pushcount
    else
        return x, r, 0, pushcount
    end
end


function weighted_ppr_push_solution(A::SparseMatrixCSC{T,Int}, alpha::Float64, seed::Int, eps::Float64) where T
    maxpush = round(Int,max(1.0/(eps*(1.0-alpha)), 2.0*10^9))
    dvec = sum(A,dims=2)
    return weighted_ppr_push(A,seed,alpha,eps,maxpush,vec(dvec),0)[1]
end


"""
Compute the set with smallest conductance 
induced by sweepcut over x.
"""
function weighted_local_sweep_cut(A::SparseMatrixCSC{T,Int}, x::Dict{Int,V}, dvec::Vector{Int}, Gvol::Int) where {T,V}
    colptr = A.colptr
    rowval = A.rowval
    nzval = A.nzval

    n = size(A,1)

    sx = sort(collect(x), by=x->x[2], rev=true)
    S = Set{Int64}()
    volS = 0.
    cutS = 0.
    bestcond = 1.
    beststats = (1,1,1,Gvol-1)
    bestpre = 0
    for (i, p) in enumerate(sx)
        if i == n
            break
        end
        u = p[1] # get the vertex
        #volS += colptr[u+1] - colptr[u]
        volS += dvec[u]

        push!(S, u)
        for nzi in colptr[u]:(colptr[u+1] - 1)
            v = rowval[nzi]
            ew = nzval[nzi]

            if v == u
                continue
            end
            if v in S
                cutS -= ew
            else
                cutS += ew
            end
        end
        if cutS/min(volS,Gvol-volS) <= bestcond
            bestcond = cutS/min(volS,Gvol-volS)
            size = length(S)
            bestpre = i 
            if Gvol - volS < volS
                size = n - size
            end
            beststats = (cutS,size,volS,Gvol-volS)
        end
    end
    bestset = Set{Int64}()
    for (i, p) in enumerate(sx)
        u = p[1]
        push!(bestset, u)
        if i == bestpre
            break
        end
    end
    return bestset, bestcond, beststats
end


"""
Compute the piecewise lower bound for conductance of 
sets induced by sweepcut over x
"""
function weighted_local_sweep_cut_curve(A::SparseMatrixCSC{T,Int}, x::Dict{Int,V}, dvec::Vector{Int}, Gvol::Int) where {T,V}
    colptr = A.colptr
    rowval = A.rowval
    nzval = A.nzval

    n = size(A,1)

    sx = sort(collect(x), by=x->x[2], rev=true)
    S = Set{Int64}()
    volS = 0.
    cutS = 0.
    bestcond = 1.
    beststats = (1,1,1,Gvol-1)
    volS_list = Vector{Int64}()
    condS_list = Vector{Float64}()
    stats_list = Vector{Tuple}()
    for p in sx
        if length(S) == n-1
            break
        end
        u = p[1] # get the vertex
        #volS += colptr[u+1] - colptr[u]
        volS += dvec[u]

        for nzi in colptr[u]:(colptr[u+1] - 1)
            v = rowval[nzi]
            ew = nzval[nzi]

            if v in S
                cutS -= ew
            else
                cutS += ew
            end
        end
        push!(S,u)
        minside_vol = min(volS, Gvol - volS)
        push!(volS_list, minside_vol)
        push!(condS_list, cutS / minside_vol)
        size = length(S)
        if volS > Gvol - volS
            size = n - size
        end
        push!(stats_list, (cutS, size, volS, Gvol - volS))
    end
    ord = sortperm(volS_list)
    volS_curve = Vector{Int64}()
    condS_curve = Vector{Float64}()
    stats_curve = Vector{Tuple}()
    for i = length(ord):-1:1
        j = ord[i]
        if length(volS_curve) == 0 || (condS_list[j] < condS_curve[end])
            push!(volS_curve, volS_list[j])
            push!(condS_curve, condS_list[j])
            push!(stats_curve, stats_list[j])
        end
    end
    return volS_curve, condS_curve, stats_curve
end


function weighted_degree_normalized_sweep_cut!(A::SparseMatrixCSC{T,Int}, x::Dict{Int,V}, dvec::Array{Int}, Gvol::Int) where {T,V}
    for u in keys(x)
        x[u] = x[u]/dvec[u]
    end

    return weighted_local_sweep_cut(A,x,dvec,Gvol)
end


function weighted_degree_normalized_sweep_cut_curve!(A::SparseMatrixCSC{T,Int}, x::Dict{Int,V}, dvec::Array{Int}, Gvol::Int) where {T,V}
    for u in keys(x)
        x[u] = x[u]/dvec[u]
    end

    return weighted_local_sweep_cut_curve(A,x,dvec,Gvol)
end


function seed_set_grow_one(A::SparseMatrixCSC{T,Int},
        seed::Int, alpha::Float64, eps::Float64) where T
    maxpush = round(Int,max(1.0/(eps*(1.0-alpha)), 2.0*10^9))
    dvec = vec(sum(A,dims=2))
    @assert eltype(dvec)==Int
    Gvol = sum(dvec)
    ppr = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec, 0)[1]
    return weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)
end


struct setstats
    size::Int
    volume_seed::Int #
    cut::Int
    seed::Int
    volume_other::Int # the volume on the other side
    cond::Float64
    support::Int # total vertices evaluated
    work::Int # total number of edges "pushed"
end


function setstats()
    return setstats(0, 0, 0, 0, 0, 1.0, 0, 0)
end


"""
`weighted_ncp`
--------------

Compute the NCP of a weighted graph. This runs a standard
personalized PageRank-based NCP except on a graph where
the edges are weighted. The weighted edges must be integers
at the moment.

Usage
-----
weighted_ncp(A)
weighted_ncp(A; ...)
- `alpha::Float64` default value 0.99, the pagerank value of alpha
- `maxcond::Float64` ignore clusters with conductance larger than maxcond
- `minsize::Int` the minimum size cluster to consider
- `maxvol::Float64' the maximum volume (if maxvol <= 1, then it's a ratio, if
    maxvol > 1, then it's a total number of edges)
- `maxvisits::Int` don't start a new cluster from a node if it's
  already been in k clusters
Returns
-------
The function returns an Array of setstats
"""


function parallel_weighted_ncp(A::SparseMatrixCSC{Int,Int};
        alpha::Float64=0.99,
        maxcond::Float64=1.0,
        minsize::Int=5,
        maxvol::Float64=1.0,
        maxvisits::Int=10)

    #n = size(A,1)
    n = checksquare(A)
    visits = SharedArray{Int}((n,))

    for i in 1:n
        visits[i] = 0
    end

    eps=1.e-5

    l = ReentrantLock()

    dvec = vec(sum(A,dims=2)) # weighted degree vectors
    @assert eltype(dvec)==Int
    Gvol = sum(dvec)

    vset = randperm(n)
    sz = 10000

    ncpdata = @showprogress pmap(1:sz) do i
        begin
            # check if the node has already been handled
            v = vset[i]
            if visits[v] >= maxvisits
                return setstats()
            end

            seed = v

            maxpush = round(Int,max(1.0/(eps*(1.0-alpha)), 2.0*10^9))
            ppr = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec, 0)[1]
            bestset,bestcond,setnums =
                weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)

            volseed = setnums[2]
            volother = Gvol-volseed
            if seed âˆ‰ bestset
                volother = volseed
                volseed = Gvol - volseed
            end

            lock(l)
            for u in bestset
                visits[u] += 1
            end
            unlock(l)

            # todo add support/work
            return setstats(length(bestset), volseed, setnums[1], seed, volother, bestcond, 0,0)
        end
    end
    return ncpdata
end


function create_ncpdata()
    return DataFrame(seed = Int64[], eps = Float64[], size=Int64[],
                    cond = Float64[], cut = Float64[], volume_seed = Int64[],
                    volume_other = Int64[], ComputeTime = Float64[])
end


function ncp_entry(n, seed, eps, bestset, bestcond, setnums, dt)
    setsize = setnums[2]
    volseed = setnums[3]
    volother = setnums[4]

    return [seed, eps, setsize,
            bestcond, setnums[1],
            volseed, volother, dt]
end


function serial_weighted_ncp(A::SparseMatrixCSC{Int,Int};
        alpha::Float64=0.99,
        maxcond::Float64=1.0,
        minsize::Int=5,
        maxvol::Float64=1.0,
        maxvisits::Int=10,
        maxlarge::Int=10,
        largethresh::Float64=0.8,
        type::Int=1,
        timelimit::Float64=3600.0,
        )

    n = checksquare(A)

    epsseq = [2,5,10,25,50,100]
    epsvals = [0.01;1.0./(100*epsseq); 1.0./(10000*epsseq); 1.0e-7; 1.0e-8]
    epsbig = 1.0e-4

    dvec = vec(sum(A,dims=2)) # weighted degree vectors
    @assert eltype(dvec)==Int
    Gvol = sum(dvec)
    meand = mean(dvec)

    ncpdata = create_ncpdata()

    lasteps = false

    for eps=epsvals
        println(eps)
        # reset visited
        visits = zeros(n)
        maxpush = round(Int,min(1.0/(eps*(1.0-alpha)), 2.0*10^9))
        if maxpush >= 100*Gvol
            # these are getting too big, let's stop here
            lasteps = true
            break
        end
        vset = randperm(n)
        @assert n > 10 "graph size is too small(<10)"
        dt = 0
        if timelimit > 0.0
            # estimate exec time
            for i = 1:10
                seed = vset[i]
                if type == 2 
                    dt += @elapsed begin
                        ppr = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec,0)[1]    
                        bestset,bestcond,setnums = weighted_degree_normalized_sweep_cut_curve!(A,ppr,dvec,Gvol)
                    end
                else
                    dt += @elapsed begin  
                        ppr = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec,0)[1]    
                        bestset,bestcond,setnums = weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)
                    end
                end
            end
            dtpersample = dt / 10
            nsamples = min(n, Int(floor(timelimit / dtpersample)))
            vset = vset[1:nsamples]
        else
            if maxpush*meand^2 > Gvol
                vset = vset[1:min(ceil(Int64,0.1*n),10^5)]
                if maxpush*meand > 50 * Gvol
                    vset = vset[1:min(ceil(Int64,0.1*length(vset)),)]
                end
            end
        end
        nlarge = 0
        for v=vset # randomize the order
            seed = v
            if visits[v] >= maxvisits
                continue
            end

            dt = @elapsed ppr = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec,0)[1]
            #@show (eps, v, dt)
            if type == 1
                # for each seeded PageRank, we take the set with smallest conductance
                # and we don't visit the same vertex too many times
                dt += @elapsed bestset,bestcond,setnums =
                    weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)

                if length(bestset) <= minsize
                    continue
                end
                push!(ncpdata,
                    ncp_entry(n, seed, eps, bestset, bestcond, setnums, dt))

                if length(bestset) > largethresh*n
                    nlarge += 1
                    if nlarge >= maxlarge
                        break
                    end
                end
                if eps < epsbig
                    for u in bestset
                        visits[u] += 1
                    end
                end
                #println(@sprintf("%8.3e  %7i  %8i  %8i  %5.3f  %6.2f",
                #    eps, ncpdata[!,:size][end], ncpdata[!,:volume_seed][end], ncpdata[!,:volume_other][end], bestcond, dt))
            elseif type == 2
                # for each seeded PageRank, we take a list of sets with small conductance
                dt += @elapsed volS_curve, condS_curve, stats_curve =
                    weighted_degree_normalized_sweep_cut_curve!(A,ppr,dvec,Gvol)
                for i = 1:length(volS_curve)
                    if volS_curve[i] < minsize 
                        continue
                    end
                    push!(ncpdata, [seed, eps, stats_curve[i][2], condS_curve[i],
                                    stats_curve[i][1], stats_curve[i][3], stats_curve[i][4], dt]) 
                end
            else
                # same with type 1 but without visiting constraint
                dt += @elapsed bestset,bestcond,setnums =
                    weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)

                push!(ncpdata,
                    ncp_entry(n, seed, eps, bestset, bestcond, setnums, dt))

            end

        end

        if lasteps
            break
        end
    end
    return ncpdata
end


function test_delocalization(A::SparseMatrixCSC, alpha::Float64, eps::Float64, dvec::Vector{Int}, Gvol::Int)
    maxpush = round(Int,max(1.0/(eps*(1.0-alpha)), 2.0*10^9))
    ntrials = 5
    for i=1:ntrials
        seed = rand(1:size(A,1))
        ppr,r,flag = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec,round(Int,Gvol*0.8))
        #@show eps, length(ppr), length(r)
        if flag == -2
            return true
        end

        bestset,bestcond,setnums = weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)
        #@show eps, setnums, length(bestset)
        if setnums[3] > setnums[4]
            return true
        end
    end

    return false
end



function estimate_delocalization(A::SparseMatrixCSC{Int}, alpha::Float64)

    dvec = vec(sum(A,dims=2))
    Gvol = sum(dvec)

    epscur = 1.0/((1-alpha)*Gvol)
    @assert epscur > eps(1.0)

    rval = test_delocalization(A, alpha, epscur, dvec, Gvol)

    if rval
        epsrange = (epscur, 0.1)
        # already delocalized, increase eps
        for i=1:20
            epscur = epscur*2.0
            if test_delocalization(A, alpha, epscur, dvec, Gvol)
                epsrange = (epscur/2., epscur)
                break
            end
        end
    else
        # decrease eps to delocalize
        epsrange = (eps(1.), epscur)
        for i=1:20
            epscur /= 2.
            if test_delocalization(A, alpha, epscur, dvec, Gvol)
                epsrange = (epscur, epscur*2.)
                break
            end
        end
    end

    # run 5 steps of bisection
    for i=1:5
        mid = epsrange[1]*0.5 + epsrange[2]*0.5
        if test_delocalization(A,alpha,mid, dvec, Gvol)
            # midpoint is delocalized, use upper points
            epsrange = (mid, epsrange[2])
        else
            # could get bigger...
            epsrange = (epsrange[1],mid)
        end
    end

    return epsrange
end


struct NCPProblem
    """ The matrix to run on. """
    A::SparseMatrixCSC{Int,Int}
    dvec::Vector{Int}
    Gvol::Int

    """ The set of alpha values to use. """
    alphas::Vector{Float64}

    """ The largest conductance to consider reporting. """
    maxcond::Float64
    minsize::Int

    """ The maximum volume to consider in a diffusion. """
    maxvol::Float64

    """ The range of eps values to consider. """
    epsrange::Tuple{Float64,Float64}
end

"""
test
"""
function ncp_problem(A::SparseMatrixCSC{Int,Int})
    epsrange = estimate_delocalization(A,0.99) # epsrange[1] is delocalized, epsrange[2] isn't
    dvec = vec(sum(A,dims=2))
    Gvol = sum(dvec)

    return NCPProblem(A, dvec, Gvol, [0.99], 1., 5, 1., epsrange)
end

randlog(a,b) = exp(rand(Float64)*(log(b)-log(a))+log(a))

function random_sample_ncp(p::NCPProblem,N::Int)
    n = size(p.A,1)
    N = min(N,n)
    verts = randperm(n)
    ncpdata = create_ncpdata()

    for i=1:N
        seed = verts[i]
        epsmax = 1/(p.dvec[seed]+1) # just go beyond trivial work
        eps = randlog(p.epsrange[1], epsmax)
        for alpha in p.alphas
            maxpush = round(Int,max(1.0/(eps*(1.0-alpha)), 2.0*10^9))
            dt = @elapsed ppr = weighted_ppr_push(p.A,seed,alpha,eps,maxpush,p.dvec,0)[1]
            dt += @elapsed bestset,bestcond,setnums =
                weighted_degree_normalized_sweep_cut!(p.A,ppr,p.dvec,p.Gvol)

            push!(ncpdata,
                ncp_entry(n, seed, eps, bestset, bestcond, setnums, dt))

            println(@sprintf("%8.3e  %7i  %8i  %8i  %5.3f  %6.2f",
                eps, ncpdata[:size][end],  ncpdata[:volume_seed][end],  ncpdata[:volume_other][end], bestcond, dt))

        end
    end

    return ncpdata
end


function make_ACL_jobs(n, jobs, nworkers)
    vsets = [Int64[] for i =1:nworkers] 
    vperm = randperm(n)
    for i in 1:n
        push!(vsets[(i % nworkers) + 1], vperm[i])
    end
    for i in 1:nworkers
        put!(jobs, vsets[i])
    end
    for _ in 1:nworkers
        put!(jobs, [])
    end
end


function solve_ACL_one_worker(A::SparseMatrixCSC{Int, Int}, vset::Vector{Int},
    alpha = 0.99, epsvals=[], setsizes=[], type=2, minvol=10)
    n = checksquare(A)

    if length(epsvals) == 0 || length(setsizes) === 0 
        epsseq = [2,5,10,25,50,100]
        epsvals = [0.01;1.0./(100*epsseq); 1.0./(10000*epsseq); 1.0e-7;1e-8]
        setsizes = length(vset) * ones(Int, length(epsvals))  
        for (i, eps) in enumerate(epsvals)
            if eps >= 1e-4
                setsizes[i] = min(setsizes[i], 10^5)
            elseif eps >= 1e-6
                setsizes[i] = min(setsizes[i], 5*10^3)
            else 
                setsizes[i] = min(setsizes[i], 100)
            end
        end
    end

    dvec = vec(sum(A,dims=2)) # weighted degree vectors
    @assert eltype(dvec)==Int
    Gvol = sum(dvec)

    ncpdata = create_ncpdata()

    lasteps = false

    for (i, eps) = enumerate(epsvals)
        println(eps)
        # reset visited
        maxpush = round(Int,min(1.0/(eps*(1.0-alpha)), 2.0*10^9))
        if maxpush >= 100000 * Gvol
            # these are getting too big, let's stop here
            lasteps = true
            break
        end
        # randomize the order
        seedset = randperm(length(vset))
        seeds = vset[seedset[1:setsizes[i]]]
        @assert length(seeds) == setsizes[i]
        totaldt = 0
        @show eps, length(seeds)
        for v=seeds 
            seed = v

            dt = @elapsed ppr = weighted_ppr_push(A,seed,alpha,eps,maxpush,dvec,0)[1]
            if type == 1
                # take one set per seeded PageRank
                dt += @elapsed begin 
                    bestset,bestcond,setnums =
                        weighted_degree_normalized_sweep_cut!(A,ppr,dvec,Gvol)
                    volseed = setnums[3]
                    volother = setnums[4]
                    if min(volseed, volother) < minvol
                        continue
                    end
                    push!(ncpdata,
                        ncp_entry(n, seed, eps, bestset, bestcond, setnums, dt))
                end
            else
                # take multiple sets per seeded PageRank
                dt += @elapsed begin 
                    volS_curve, condS_curve, stats_curve =
                        weighted_degree_normalized_sweep_cut_curve!(A,ppr,dvec,Gvol)
                    for i = 1:length(volS_curve)
                        volseed = stats_curve[i][3]
                        volother = stats_curve[i][4]
                        @assert min(volseed, volother) == volS_curve[i]
                        if volS_curve[i] < minvol 
                            continue
                        end
                        push!(ncpdata, [seed, eps, stats_curve[i][2], condS_curve[i],
                                        stats_curve[i][1], stats_curve[i][3], stats_curve[i][4], dt]) 
                    end
                end
            end
            totaldt += dt
        end
        @show eps, totaldt / length(vset)
        if lasteps
            break
        end
    end
    return ncpdata  
end

function do_ACL_jobs(
    jobs,
    results,
    A::SparseMatrixCSC{Int, Int},
    alpha::Float64=0.99,
    epsvals=[],
    setsizes=[],
    type=2,
    minvol=10)
    while(true)
        vset = take!(jobs)
        if length(vset) == 0
            break
        end
        ncpdata = solve_ACL_one_worker(A, vset, alpha, epsvals, setsizes, type, minvol)
        put!(results, ncpdata)
    end
end


function bulk_local_ACL(A::SparseMatrixCSC{Int,Int};
    alpha::Float64=0.99,
    epsvals=[],
    setsizes=[],
    filename::String="livejournal.csv",
    type=2,
    minvol=10,
)
    n = size(A, 1)
    nworkers = length(workers())
    jobs = RemoteChannel(()->Channel{Vector{Int64}}(nworkers + nworkers))
    results = RemoteChannel(()->Channel{DataFrame}(nworkers))
    make_ACL_jobs(n, jobs, nworkers)
    for p in workers()
        remote_do(do_ACL_jobs, p, jobs, results, A, alpha, epsvals, setsizes, type, minvol)
    end
    all_ncp = create_ncpdata()
    while nworkers > 0
        ncp = take!(results)
        nworkers -= 1
        all_ncp = vcat(all_ncp, ncp)
    end
    close(jobs)
    close(results)
    CSV.write(filename, all_ncp)
    return all_ncp
end

#=




function serial_weighted_ncp(A::SparseMatrixCSC{Int,Int};
        alpha::Float64=0.99,
        maxcond::Float64=1.0,
        minsize::Int=5,
        maxvol::Float64=1.,
        maxvisits::Int=10,
        maxlarge::Int=10,
        largethresh::Float64=0.8)
end

=#
function test_progress()
end 

end # end module
